{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"121.85px","left":"491.667px","right":"20px","top":"120px","width":"341.667px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rfp1U9PMeQU1"},"source":["\n","# TP CNN\n","### Diane LINGRAND \n","\n","diane.lingrand@univ-cotedazur.fr   \n","Polytech - SI4 - 2021"]},{"cell_type":"markdown","metadata":{"id":"uo8ucmMpgEp9"},"source":["## Introduction"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:15:27.613140Z","start_time":"2021-11-27T16:15:27.605763Z"},"id":"68uwyRRi4BMK"},"source":["from IPython.display import Image\n","import tensorflow as tf\n","print(tf.__version__)\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Activation\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZMrLWuFy9gn7"},"source":["**The GPU**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TO0xCfDwz-zT"},"source":["To enable GPU backend in Google colab for your notebook:\n","\n","1.   Runtime (top left corner) -> Change runtime type\n","2.   Put GPU as \"Hardware accelerator\"\n","3.   Save.\n","\n","Or run the next cell:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:01:19.986158Z","start_time":"2021-11-27T16:01:19.712649Z"},"id":"0HFjESp9DQoM"},"source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hHU7TK6FrHao"},"source":["## Convolutional Neural Networks (CNN)"]},{"cell_type":"markdown","metadata":{"id":"eP0L5uYbfAzC"},"source":["Derived from the MLP, a convolutional neural network (CNN) is a type of artificial neural network that is specifically designed to process **pixel data**.  The layers of a CNN consist of an **input layer**, an **output layer** and **hidden layers** that can include **convolutional layers**, **pooling layers**, **fully connected layers** and **normalization layers**. It exists a lot of techniques to optimize CNN, like for example the dropout."]},{"cell_type":"markdown","metadata":{"id":"bfCDrvt8qQPY"},"source":["### Loading the dataset\n","In this part, we will use photographies of animals from the kaggle dataset [animals-10](https://www.kaggle.com/alessiocorrado99/animals10). Please connect to their site before loading the dataset from this [zip file](http://www.i3s.unice.fr/~lingrand/raw-img.zip). Decompress the zip file on your disk.\n","\n","If you are using google colab, there is no need to download the dataset because I have a copy on my drive. You just need add to your drive this shared folder: https://drive.google.com/drive/folders/15cB1Ky-7OTUqfcQDZZyzc5HArt0GA6Sm?usp=sharing\n","You need to click on the link and click on \"Add shortcut to Drive\" and then select \"My Drive\"."]},{"cell_type":"code","metadata":{"id":"KqQrRVZrDQoQ"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RM78thFqt-ZY"},"source":["To feed the data to a CNN, we need to shape it as required by Keras. As input, a 2D convolutional layer needs a **4D tensor** with shape: **(batch, rows, cols, channels)**. Therefore, we need to precise the \"channels\" axis, which can be seen as the number of level of color of each input: 3 channels in our case. We will fix the dimension of images according to the VGG-16 network: (224, 224).\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:17:51.518829Z","start_time":"2021-11-27T16:17:51.401963Z"},"id":"i_yPS5rYF1Sk"},"source":["from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D, Flatten\n","from sklearn.metrics import confusion_matrix, plot_confusion_matrix, f1_score\n","import tensorflow.keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","import numpy as np\n","import glob\n","# when processing time is long, it's nice to see the progress bar\n","#!pip install tqdm\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzHkKLqlZPn3"},"source":["### loading train data\n","\n","Please read the code before running any of the cells!"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:18:22.601760Z","start_time":"2021-11-27T16:18:16.102817Z"},"id":"xkVd3v4N3LnB"},"source":["#datasetRoot='/home/lingrand/Ens/MachineLearning/animals/raw-img/'\n","#datasetRoot='/whereYouPutTheImages/'\n","datasetRoot='/content/drive/My Drive/raw-img/'\n","# I suggest to reduce the number of classes for a first trial. \n","# If you finish this notebook before the end of the course, you can add more classes (and images per class).\n","classes = ['mucca', 'elefante', 'gatto', 'cavallo', 'scoiattolo', 'ragno', 'pecora', 'farfalla', 'gallina', 'cane']\n","nbClasses = len(classes)\n","\n","#training data\n","\n","rootTrain = datasetRoot+'train/'\n","classLabel = 0\n","reducedSizePerClass = 200 #in order to reduce the number of images per class\n","totalImg = nbClasses * reducedSizePerClass\n","xTrain = np.empty(shape=(totalImg,224,224,3))\n","yTrain = []\n","first = True\n","i= 0\n","for cl in classes:\n","    listImages = glob.glob(rootTrain+cl+'/*')\n","    yTrain += [classLabel]*reducedSizePerClass #len(listImages) # note that here ...\n","    for pathImg in tqdm(listImages[:reducedSizePerClass]): # and here, we have reduced the data to be loaded (only 1000 per class)\n","        img = image.load_img(pathImg, target_size=(224,224))\n","        im = image.img_to_array(img)\n","        im = np.expand_dims(im, axis=0)\n","        im = preprocess_input(im)\n","        xTrain[i,:,:,:] = im\n","        i += 1\n","    classLabel += 1\n","print(len(yTrain))\n","print(xTrain.shape)\n","yTrain = tensorflow.keras.utils.to_categorical(yTrain, nbClasses)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kig9dk-GExjU"},"source":["**[TO DO - Students] What is the dimension of xTrain ? What do those dimensions represent ?**\n"]},{"cell_type":"markdown","metadata":{"id":"tEpDCqyOPDtT"},"source":["**[TO DO - Students] Complete the following code to plot a few training images**\n"]},{"cell_type":"code","metadata":{"id":"wvzIO2K55jtz"},"source":["import matplotlib.pyplot as plt\n","\n","square = 8\n","ix = 1\n","fig, axs = plt.subplots(square, square, figsize=(20, 20))\n","for i in range(square):\n","    for j in range(square):\n","        # specify subplot and turn of axis\n","        ax = axs[i,j]\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        im = xTrain[ix][:,:,...]\n","        ax.imshow(...)\n","        ix += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56bq9oXanGUm"},"source":["In order to speed-up the time spent on this part of the lab, you may have noticed that we reduced the number of classes and the number of images per class. You can change these few lines of code if you want to work on the whole dataset."]},{"cell_type":"markdown","metadata":{"id":"boNapUgGaEMj"},"source":["### loading test data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:39:11.565427Z","start_time":"2021-11-27T16:39:05.538947Z"},"id":"Zwi5TBlKajtt"},"source":["#you need to use the same classes for the test dataset than for the train dataset\n","rootTest = datasetRoot+'test/'\n","classLabel = 0\n","\n","totalTestImg = 0\n","for cl in classes:\n","    totalTestImg += len(glob.glob(rootTest+cl+'/*'))\n","\n","print(\"There are \",totalTestImg, \" images in test dataset.\")\n","xTest = np.empty(shape=(totalTestImg,224,224,3))\n","yTest = []\n","i = 0\n","\n","for cl in classes:\n","    listImages = glob.glob(rootTest+cl+'/*')\n","    yTest += [classLabel]*len(listImages)\n","    for pathImg in listImages:\n","        img = image.load_img(pathImg, target_size=(224, 224))\n","        im = image.img_to_array(img)\n","        im = np.expand_dims(im, axis=0)\n","        im = preprocess_input(im)\n","        xTest[i,:,:,:] = im \n","    classLabel += 1\n","print(len(yTest))\n","print(xTest.shape)\n","yTest = tensorflow.keras.utils.to_categorical(yTest, nbClasses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WYwcn50GDQoV"},"source":["## Build your own CNN network"]},{"cell_type":"markdown","metadata":{"id":"B2RVxCXODQoW"},"source":["**[TO DO - Students] Start with the simplest CNN: 1 conv2D layer + 1 pooling + 1 dense layer. Fill the gaps and explain the parameters of the MaxPooling2D layer**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:33:54.152252Z","start_time":"2021-11-27T16:33:54.099134Z"},"id":"7bPi5XJMDQoW"},"source":["model = Sequential()\n","model.add(Conv2D(32,(3,3),padding='same',activation='relu', input_shape=...))\n","model.add(MaxPooling2D(pool_size=(4, 4), strides=4, padding='same'))\n","model.add(Flatten())\n","model.add(Dense(..., activation=...))\n","model.compile(optimizer='rmsprop',loss=..., metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iiv8G8GEDQoX"},"source":["Let's look at the dimension of all inputs and outputs:"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:33:56.687308Z","start_time":"2021-11-27T16:33:56.674345Z"},"id":"VEgr3tjtDQoX"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL5YHt4aDQoX"},"source":["**[TO DO - Students] Train and test this network.**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T16:36:46.534902Z","start_time":"2021-11-27T16:34:01.881134Z"},"id":"LRYWdVblDQoY"},"source":["## Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ykyptFKQWPI"},"source":["**[TO DO - Students] Plot the training metrics (loss and accuracy). Test the model on the test data and compare the confusion matrix on the test data and train data**"]},{"cell_type":"code","metadata":{"id":"bD5pZmCeWx9K"},"source":["# Plot history\n","f, (ax1, ax2) = plt.subplots(1,2)\n","ax1.plot(history.history['loss'], label='train')\n","ax1.plot(history.history['val_loss'], label='val')\n","ax1.legend()\n","ax2.plot(history.history['accuracy'], label='train')\n","ax2.plot(history.history['val_accuracy'], label='val')\n","ax2.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-11-27T17:26:29.630455Z","start_time":"2021-11-27T17:26:21.961418Z"},"id":"MRz1Io7LDQoY"},"source":["# for you !\n","score = model.evaluate(xTest,yTest)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n","\n","ypred = np.argmax(model.predict(xTest), axis=1)\n","print(\"F1 score: \", f1_score(ypred,np.argmax(yTest,axis=1),average='micro'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpHpSYw84-Qa"},"source":["Visualize the confusion matrix on the test dataset for this model"]},{"cell_type":"code","metadata":{"id":"xMILyY-f5Adx"},"source":["y_pred = model.predict(xTrain)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLoJMJJ95gw1"},"source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","le.fit(classes)\n","\n","cm = confusion_matrix(le.inverse_transform(np.argmax(yTrain, axis=1)), \n","                      le.inverse_transform(np.argmax(y_pred, axis=1)), \n","                      labels=classes)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                              display_labels=classes)\n","disp.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"solution":"hidden","solution_first":true,"id":"q-uzAlM6DQoZ"},"source":["How is the accuracy or F1-measure on the test dataset?\n","\n","Are you satisfied by the performances?\n","\n","Try to modify the architecture (add layers) and some of the parameters."]},{"cell_type":"markdown","metadata":{"id":"HDg_e-Ax3uci"},"source":["### About Dropout \n","\n","*Study this part only if you have time for it. It concerns the previous network but prefer to study first part II and come back here after.*\n","\n","Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By “ignoring”, I mean these units are not considered during a particular forward or backward pass.\n","\n","Why use dropout ? A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to overfitting of training data."]},{"cell_type":"markdown","metadata":{"id":"eC5ct2L36pKC"},"source":["**Let's add dropout and activation functions to the network!**"]},{"cell_type":"code","metadata":{"id":"y20GTiux6uy9"},"source":["from tensorflow.keras.layers import Dropout\n","\n","model = Sequential(name='MLP model with dropout') \n","\n","model = Sequential()\n","model.add(Conv2D(256,(3,3),activation='relu',input_shape=(224,224,3)))\n","model.add(GlobalAveragePooling2D())\n","model.add(Dense(200,activation='relu'))\n","# adding dropout to the previous layer\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(nbClasses, activation='softmax'))\n","\n","model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MNDfiTEQ1UZ"},"source":["**[TO DO - Students] Plot the training metrics (loss and accuracy). Test the model on the test data and compare the confusion matrix on the test data and train data**"]},{"cell_type":"markdown","metadata":{"id":"uLv2HhmnDQoa"},"source":["## Using a pre-learned network"]},{"cell_type":"markdown","metadata":{"id":"Og1KZntwavZT"},"source":["### loading VGG-16 description part and adding layers to build our own classification network"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-12T14:11:09.440297Z","start_time":"2021-10-12T14:11:09.179442Z"},"id":"ofrQr-x_a-Bi"},"source":["VGGmodel = VGG16(weights='imagenet', include_top=False)\n","#features = VGGmodel.predict(xTrain)\n","#print(features.shape)\n","VGGmodel.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ht--ZKNBTQ9x"},"source":["**[TO DO - Students] What is the goal of the include_top=false parameter and adapt the model to our classification model by filling the gaps of the following cell**"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-12T14:11:36.842555Z","start_time":"2021-10-12T14:11:36.806935Z"},"id":"SXEJY2xaDQoa"},"source":["# we will add layers to this feature extraction part of VGG network\n","m = VGGmodel.output\n","# we start with a global average pooling\n","m = GlobalAveragePooling2D()(m)\n","# and add a fully-connected layer\n","m = Dense(1024, activation='relu')(m)\n","# finally, the softmax layer for predictions (we have nbClasses classes)\n","predictions = Dense(..., activation=...)(m)\n","\n","# global network\n","model = Model(inputs=VGGmodel.input, outputs=predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6POPQoXcbPuc","solution":"hidden","solution_first":true},"source":["Can you display the architecture of this entire network?"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-12T14:11:57.817996Z","start_time":"2021-10-12T14:11:57.803351Z"},"id":"IrrUmVsoDQob"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNz7wtvKTvw4"},"source":["**[TO DO - Students] What would happen if we ran model.fit now ? Make it so that the training will only train the new layers and train the model.**"]},{"cell_type":"code","metadata":{"id":"PrLUhA2-b8Fv"},"source":["## Your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuW9wsU6Lzjj"},"source":["Some classes are not predicted because we did not shuffle the data and every samples of some datasets are part of the validation set."]},{"cell_type":"markdown","metadata":{"id":"4hgDguiNcNXZ"},"source":["### fine-tune the network"]},{"cell_type":"markdown","metadata":{"id":"L9jmknEpcT1_"},"source":["Fine-tune the entire network if you have enough computing ressouces, otherwise, carefully choose the layers you want to fine-tune."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-10-12T14:13:56.365304Z","start_time":"2021-10-12T14:13:56.350142Z"},"id":"cRr97-2yc6kZ"},"source":["for i, layer in enumerate(VGGmodel.layers):\n","   print(i, layer.name)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v4eCse6wku5Q"},"source":["In this example, we will fine-tune the last convolution block starting at layer number 15 (block5_conv)."]},{"cell_type":"code","metadata":{"id":"C17ts6kllGUr"},"source":["from tensorflow.keras.optimizers import RMSprop\n","for layer in model.layers[:11]:\n","   layer.trainable = False\n","for layer in model.layers[11:]:\n","   layer.trainable = True\n","#need to recompile the network\n","model.compile(optimizer=RMSprop(learning_rate=0.0001), loss='categorical_crossentropy',metrics=['accuracy'])\n","#and train again ...\n","model.fit(xTrain, yTrain, epochs=20, batch_size=128, validation_split=0.2, callbacks=[ourCallback],verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LrhxQiBmTZj","solution":"hidden","solution_first":true},"source":["You already know how to evaluate the performances on the test dataset and display the confusion matrix. You can also modify the code that loads the test dataset in order to reduce it's size. Let's do it!"]},{"cell_type":"code","metadata":{"id":"RHGxb-PxmiYd"},"source":["#enter here your code for evaluation of performances"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unzKoPOsgNW3","solution":"hidden","solution_first":true},"source":["You are now free to experiments changes in the network:\n","* add a dense layer\n","* modify the number of neurons in dense layer(s)\n","* change the global average polling\n","* add classes and data\n","* experiment other optimizers (SGD, Adam, ...)\n","\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"kxxB0OQFY8uY"},"source":["## Visualizing the convolution filters"]},{"cell_type":"markdown","metadata":{"id":"j94suFbG73Yk"},"source":["In this part, we'll visualize the convolution filters and their effect on the input for our previously trained model"]},{"cell_type":"markdown","metadata":{"id":"fxOf52Eq8WLG"},"source":["**[TO DO - Students] What is the following code plotting ?**"]},{"cell_type":"code","metadata":{"id":"MMqqv7GLZFX9"},"source":["layer_id = 1\n","# retrieve weights from the second hidden layer\n","filters, biases = model.layers[layer_id].get_weights()\n","# normalize filter values to 0-1 so we can visualize them\n","f_min, f_max = filters.min(), filters.max()\n","filters = (filters - f_min) / (f_max - f_min)\n","# plot first few filters\n","n_filters, ix = 6, 1\n","for i in range(n_filters):\n","\t# get the filter\n","\tf = filters[:, :, :, i]\n","\t# plot each channel separately\n","\tfor j in range(3):\n","\t\t# specify subplot and turn of axis\n","\t\tax = pyplot.subplot(n_filters, 3, ix)\n","\t\tax.set_xticks([])\n","\t\tax.set_yticks([])\n","\t\t# plot filter channel in grayscale\n","\t\tpyplot.imshow(f[:, :, j], cmap='gray')\n","\t\tix += 1\n","# show the figure\n","pyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYigzjt2Uo93"},"source":["**[TO DO - Students] Now, let's visualize the feature maps of various depths. Fill the gaps to do so.**"]},{"cell_type":"markdown","metadata":{"id":"k5EywE7I8UQ2"},"source":[]},{"cell_type":"code","metadata":{"id":"uQF3R4wtadLJ"},"source":["# redefine model to output right after the first hidden layer\n","img = xTrain[...].reshape(...)\n","\n","for layer in ['block1_conv2', 'block2_conv2', 'block3_conv1', 'block4_conv1', 'block5_conv1']:\n","    model_fm = Model(inputs=model.inputs, outputs=...)\n","    feature_maps = model_fm.predict(img)\n","    # plot all 64 maps in an 8x8 squares\n","    square = 6\n","    ix = 1\n","    fig, axs = plt.subplots(square, square, figsize=(10, 10))\n","    for i in range(square):\n","      for j in range(square):\n","        # specify subplot and turn of axis\n","        ax = axs[i,j]\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        # plot filter channel in grayscale\n","        ax.imshow(feature_maps[0, :, :, ...], cmap='gray')\n","        ix += 1\n","    # show the figure\n","    fig.suptitle(layer, fontsize=20)\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvUeiynTgVHn"},"source":["## Activation maximization"]},{"cell_type":"markdown","metadata":{"id":"PZCOk0f385II"},"source":["Another solution to interpret the inner mecanisms of the network is to use Activation Maximization. This method computes the optimal output which gives the maximum value of a particular activation. Used on the classification layers, this can give us an idea of the patterns recognized to classify a particular class.\n","\n","To do that we'll use the tf_keras_vis module."]},{"cell_type":"code","metadata":{"id":"B9TrndS7SZOx"},"source":["[i for i in range(10)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Gu8hSd-g0M5"},"source":["! pip install tf_keras_vis\n","from tf_keras_vis.activation_maximization import ActivationMaximization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CReKzPWltcY"},"source":["def loss(output):\n","  return (output[0][0], output[1][1], output[2][2])\n","\n","def model_modifier(m):\n","    m.layers[-1].activation = tensorflow.keras.activations.linear\n","\n","visualize_activation = ActivationMaximization(model, model_modifier)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e5aedUZndpL"},"source":["seed_input = tensorflow.random.uniform((3, 224, 224, 3), 0, 255)\n","activations = visualize_activation(loss, seed_input=seed_input, steps=512)\n","images = [activation.astype(np.float32) for activation in activations]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3QWPR6OUm0z2"},"source":["fig, axs = plt.subplots(1, 3, figsize=(20, 20))\n","for i in range(0, len(images)):\n","  ax = axs[i]\n","  visualization = images[i].reshape(224,224,3)\n","  visualization = (visualization - visualization.min())/(visualization.max()-visualization.min())\n","  visualization = visualization[:,:,[2,1,0]]\n","  ax.imshow(visualization)\n"],"execution_count":null,"outputs":[]}]}