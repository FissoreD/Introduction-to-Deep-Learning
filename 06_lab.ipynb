{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d43ZutdxJm-"
      },
      "source": [
        "## A short introduction to Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ESYSjZOxJnA"
      },
      "source": [
        "### 1) Part 1: implement Q-learning from scratch\n",
        "\n",
        "\n",
        "We have discussed Q-learning during the class. As you know, it is an off-policy algorithm that uses the Time Difference $\\delta_t$, which is the difference between the estimated value of $s_t$ and the better estimate $r_{t+1} + \\gamma V^\\pi (s_{t+1})$\n",
        "\n",
        "$$ \\delta_t = r_{t+1} + \\gamma V^\\pi (s_{t+1}) - V^\\pi (s_t) $$\n",
        "\n",
        "The general definition of Q-learning update rule is:\n",
        "\n",
        "$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t,a_t) ] $$\n",
        "\n",
        "\n",
        "In this part, we are going to implement Q-learning in the simple setting of a 1D grid world:\n",
        "\n",
        "![1D grid world](06_lab_1dgrid.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOR_QhexJnA"
      },
      "source": [
        "Make sure you understand:\n",
        "- the size of the grid world (= number of states)\n",
        "- the size of the action space (= number of possible actions)\n",
        "- the size of the Q-table\n",
        "- the expected reward for reaching each state\n",
        "\n",
        "The first step will be to initialize an empty Q-table, a table of rewards, a move cost, and alpha and gamma parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "sBYHYHldxJnB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# we have 2 actions : move left and move right\n",
        "nb_action = 2\n",
        "nb_state = 6\n",
        "\n",
        "# we create a matrix 6*2 to represent the value of each move at a given state\n",
        "QTable = np.zeros((nb_state,nb_action))\n",
        "\n",
        "# the tab with the representation of the 6 states (-1 for the bad end, 1 for the good end, and 0 for other states)\n",
        "reward = [-1,0,0,0,0,1 ]\n",
        "\n",
        "# cost of one move\n",
        "cost = 0.01\n",
        "\n",
        "# learning rate - should not be too high, e.g. between .5 and .9\n",
        "alpha = 0.9\n",
        "\n",
        "# discount factor that shows how much you care about future (remember 0 for myopic)\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QTable[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPp79iESCQzr",
        "outputId": "d033bf18-807e-4568-ef01-78f669316a11"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBbCIGfBxJnC"
      },
      "source": [
        "Now comes the interesting part. You need to write the main Q-learning loop.\n",
        "\n",
        "The first version will simply iterate:\n",
        "- choose an action (by looking up in the Q-table! Choose the most interesting move)\n",
        "- move\n",
        "- update the Q-table\n",
        "\n",
        "When you get this version, you can make it more complete to add the exploration/exploitation with the $\\epsilon$-greedy version, by initializing an $\\epsilon = 1$ that you decrease by e.g. 0.01 in each iteration. In your main loop, start by drawing a random number. If it is lower that $\\epsilon$, then EXPLORE (= take a random move), otherwise EXPLOIT (= choose the best move)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEFT = 0\n",
        "RIGHT = 1"
      ],
      "metadata": {
        "id": "nFOmqvM8HJic"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "vzfcSWDVxJnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "105089ce-dc39-4987-9e51-fc7f7a033a8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.     ,  0.     ],\n",
              "       [-0.909  ,  0.69461],\n",
              "       [-0.0099 ,  0.7829 ],\n",
              "       [-0.01629,  0.881  ],\n",
              "       [ 0.     ,  0.99   ],\n",
              "       [ 0.     ,  0.     ]])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# your code here\n",
        "current_state = 2\n",
        "def isStuked(reward, pos): return reward[pos] != 0\n",
        "\n",
        "def computeNextState(pos, move):\n",
        "  return pos + (1 if move else -1)\n",
        "\n",
        "def loop(nb_epochs, table, cost, alpha, gamma, reward):\n",
        "  stop = -1\n",
        "  for i in range(nb_epochs):\n",
        "    current_state = 2\n",
        "    epsilon = 1\n",
        "    nb_step = 0\n",
        "    while True:\n",
        "      epsilon = 0.5 - i * 0.001\n",
        "\n",
        "      if np.random.random() < epsilon:\n",
        "\n",
        "        move = 0 if np.random.random() > 0.5 else 1\n",
        "        next_pos = computeNextState(current_state, move)\n",
        "\n",
        "      else:\n",
        "\n",
        "        move = 0 if table[current_state, 0] > table[current_state, 1] else 1\n",
        "        next_pos = computeNextState(current_state, move)\n",
        "        table[current_state, move] += alpha * (\n",
        "            (reward[next_pos]-cost) + gamma * max(\n",
        "                table[next_pos, 0], table[next_pos, 1]) - \n",
        "                table[current_state, move])\n",
        "        \n",
        "      nb_step += 1\n",
        "      if nb_step == stop or isStuked(reward, next_pos):\n",
        "        break\n",
        "      current_state = next_pos\n",
        "\n",
        "QTable = np.zeros((nb_state,nb_action))\n",
        "loop(10000, QTable, cost, alpha, gamma, reward)\n",
        "QTable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SujWqoAExJnD"
      },
      "source": [
        "### 1) Part 2 (optional): FrozenLake with Open AI Gym\n",
        "\n",
        "In this part, we will use [Gym](https://www.gymlibrary.dev) framework to play with Q-learning and SARSA (without implementing them).\n",
        "\n",
        "You can browse the many example provided by Gym. We will use the [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/#frozen-lake) game.\n",
        "\n",
        "Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake. Check the page for more information!\n",
        "\n",
        "![Frozen lake](https://www.gymlibrary.dev/_images/frozen_lake.gif)\n",
        "\n",
        "First, you'll need to install the library -- if not already installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "cdBenqTmxJnE"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import gym\n",
        "except:\n",
        "    !pip install gym\n",
        "    import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBAhmPvxJnE"
      },
      "source": [
        "Using Gym makes it very easy to implement RL algorithms.\n",
        "\n",
        "For Frozen Lake, you'll need to initialize the environement with:\n",
        "\n",
        "`env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)`\n",
        "\n",
        "As stated in this [nice blog](https://blog.paperspace.com/getting-started-with-openai-gym/), the basic structure of the environment is described by the `observation_space` and the `action_space` attributes of the Gym `Env` class.\n",
        "\n",
        "The `observation_space` defines the structure as well as the legitimate values for the observation of the state of the environment. The observation can be different things for different environments. The most common form is a screenshot of the game. There can be other forms of observations as well, such as certain characteristics of the environment described in vector form.\n",
        "\n",
        "Similarly, the `Env` class also defines an attribute called the `action_space`, which describes the numerical structure of the legitimate actions that can be applied to the environment.\n",
        "\n",
        "Now, for the Frozen Lake, we will start by defining the environment, and initializing all needed variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "XffWq8QUxJnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6d9e1b-bfe8-4703-fac6-c811637fc300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "epsilon = 1\n",
        "total_episodes = 1000\n",
        "max_steps = 1000\n",
        "\n",
        "lr_rate = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZqYeslnxJnF"
      },
      "source": [
        "We can now define a function that will choose an action given a state.\n",
        "\n",
        "You can directly implement the $\\epsilon$-greedy version. Note that `env.action_space.sample()` will select a ramdom action."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([env.action_space.sample() for i in range(9)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oXLS1L-WlpH",
        "outputId": "631d4789-bbff-4bfc-b999-f451c98f1249"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 3, 1, 0, 1, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "zwWo8nc0xJnF"
      },
      "outputs": [],
      "source": [
        "def choose_action(state):\n",
        "  if np.random.random() < epsilon:\n",
        "    return env.action_space.sample()\n",
        "  else:\n",
        "    return Q[state].argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldB54gn8xJnG"
      },
      "source": [
        "Now comes the Q-learning core learning part.\n",
        "\n",
        "The following function will update the Q-table, given the inital state, the target state after moving, the reward, and the selected action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "eltNE3LRxJnG"
      },
      "outputs": [],
      "source": [
        "def learnWithQLearning(state, state2, reward, action):\n",
        "    # your code here\n",
        "    Q[state, action] = alpha * (\n",
        "            (reward-cost) + gamma * max(\n",
        "                Q[state2, action], Q[state2, action]) - \n",
        "                Q[current_state, action]) + Q[state, action]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAIXWiiHxJnG"
      },
      "source": [
        "Note that we also can easily define the SARSA algorithm, very similar to Q-learning, that needs to know one step further: the next action.\n",
        "\n",
        "(You may come back to this point later, and first finish the main iteration loop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "axfTHY0nxJnG"
      },
      "outputs": [],
      "source": [
        "def learnWithSARSA(state, state2, reward, action, action2):\n",
        "    # your code here\n",
        "  ...\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqbHTi1dxJnG"
      },
      "source": [
        "The main iteration loop is given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "15cF06mZxJnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3cd9e9-50e1-46b1-b3e7-96e7c7f62e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.35535731e+047  5.05651297e+159 -4.63223834e+028 -1.96163034e+054]\n",
            " [ 3.55470458e+021  7.39442559e+158 -6.55656196e+000 -1.47964194e+022]\n",
            " [-4.21645318e+019 -3.09576734e+018 -3.70010831e+000 -4.17688378e+011]\n",
            " [ 1.48246867e+032  2.68983399e+018  7.19963890e-002  1.14596031e+015]\n",
            " [ 1.30873057e+026  6.48044430e+080 -4.76221091e+013 -1.73212039e+024]\n",
            " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
            " [-4.59728296e+019  2.73098356e+020 -4.13838399e+000  5.80472573e+012]\n",
            " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
            " [ 9.55305600e+021  2.17618573e+051 -1.37157189e+012 -4.50179686e+015]\n",
            " [-2.33900748e+017 -3.39012571e+017  3.05948602e+012 -1.99040289e+015]\n",
            " [-8.82271899e+017 -1.04492488e+018  3.17581487e+011 -7.93673870e+014]\n",
            " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
            " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]\n",
            " [-1.74539607e+014 -2.33462333e+018  1.06785196e+013 -5.11214644e+014]\n",
            " [-2.10766722e+017 -4.54020237e+018  3.47641353e+011 -6.38899336e+014]\n",
            " [ 0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000]]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "# Start\n",
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    #print(\"Episode \",episode)  \n",
        "    epsilon = 1 \n",
        " \n",
        "    while t < max_steps:\n",
        "      # env.render(mode = \"human\")\n",
        "      action = choose_action(state)  \n",
        "      state2, reward, done, info = env.step(action)\n",
        "      epsilon = 0.5 - episode * 0.001\n",
        "\n",
        "      # your code here to choose an action and to update the table (= learn)\n",
        "      \n",
        "      learnWithQLearning(state, state2, reward, action)\n",
        "      state = state2\n",
        "\n",
        "      t += 1\n",
        "      if done:\n",
        "          break\n",
        "      \n",
        "      # Uncomment for better visualization\n",
        "      # time.sleep(0.1)\n",
        "    \n",
        "\n",
        "print(Q)\n",
        "\n",
        "# with open(\"frozenLake_qTable.pkl\", 'wb') as f:\n",
        "#     pickle.dump(Q, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40pijVdlbf0A"
      },
      "execution_count": 210,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}